# Java中的二进制与编码

## 整数的二进制与位运算

**十进制的123 = 1乘以10的二次方 + 2乘以10的一次方 + 3乘以10的0次方**

每个位置都有**位权**，从左到右，第一位为1，第二位为10，第三位为100.

### 正整数的二进制

| 二进制 | 十进制 | 计算过程 | 
| :---- | :----- | :------ |
| 10 | 2 | 1乘以2的1次方 + 0乘以2的0次方 | 
| 11 | 3 | 1乘以2的1次方 + 1乘以2的0次方 |
| 111 | 7 | 1乘以2的2次方 + 1乘以2的1次方 + 1乘以2的0次方 |
| 1010 | 10 | 1乘以2的3次方 + 0乘以2的2次方 + 1乘以2的1次方 + 0乘以2的0次方 | 

### 负整数的二进制

**二进制使用最高位表示符号位，用1表示负数，用0表示正数。**

负数的二进制表示就是对应的正数的补码表示。

正数有4中类型：**byte, short, int, long**，分别占**1, 2, 4, 8个字节**，及分别占**8, 16, 32, 64位**，所以每一种类型的符号位都是最左边的一位。

**1. 原码表示法：**

byte a = -1

将最高位变为1，二进制是10000001

**2. 补码表示法：**

byte a = -1

1的原码表示法是00000001，取反就是11111110，然后再加1，即11111111

**3. 给定一个负数的二进制求十进制**

10010010

首先取反，得到01101101，然后加1，得到0110110，它的十进制为110，所以原值就是-100.

**4. 计算机只能做加法**

```java
1  ->  00000001
-1 ->  11111111
+ -------------
0      00000000
```

**5. 当计算结果超出计算范围**

```java
127 -> 01111111
1   -> 00000001
+ -------------
-128   10000000
```

## 浮点数的二进制表示

小数计算不准确

```java
float f = 0.1f * 01.f;
System.out.println(f); //f is 0.010000001
```

**计算机是用一种二进制格式存储小数的，这个二进制格式不能精确的表示0.1，只能表示一个非常接近0.1但又不等于0.1的一个数。**

如果需要高精度的计算，使用Java中的`BigDecimal`，运算更准确，但是效率比较低。

## 十六进制

二进制写起来太长，**可以将4个二进制位简化为一个0~15的数，10-15用A~F表示**

| 二进制 | 十进制 | 十六进制 | 
| :---- | :----- | :------ |
| 1010 | 10 | A | 
| 1011 | 11 | B |
| 1100 | 12 | C |
| 1101 | 13 | D |
| 1110 | 14 | E |
| 1111 | 15 | F |

- 可以使用十六进制直接写常量数字，在数字前面加0x。

```java
int a = 123;
int a = 0x7B;
```

## 位运算

**二进制级别的操作**  

- **左移(<<)**: 向左移动，右边的补0，高位的舍弃。
- **无符号右移(>>>)**: 向右移动，右边的舍弃，左边补0.
- **有符号右移(>>)**: 向右移动，右边的舍弃，左边补什么取决于原来的最高位。

**逻辑运算**

- 按位与&：两位都位1才为1
- 按位或|：只有有一位为1，就为1
- 按位取反~：1变为0，0变为1
- 按位异或^：相异为真，相同为假

## 字符的编码

**编码有两大类：**

- Unicode编码
- 非Unicode编码（ASCII, ISO8859-1, Windows-1252, GB2312, GBK, GB18030, Big5）

### 非Unicode编码

1. ASCII

**American Standard Code For Information Interchange**

美国大概只需要128个字符，所以规定了128个字符的二进制表示法，称为ASCII编码。

128个字符用7位刚好表示，计算机存储的最小单元是byte，即8位，ASCII码中最高位置位0.

为了保持与ASCII码的兼容性，一般都是将最高位设置为1。**当最高位为0时表示ASCII码，当为1时就是各个国家自己的字符**

2. ISO8859-1

ISO8859-1又称Latin-1

0~127与ASCII码一样，128-255规定了不同的含义。

3. Windows-1252

与ISO8859-1基本一样，区别在于128~159.

4. GB2312

美国和西欧字符用一个字节就够了，但是中文不够。中文的第一个标准就是GB2312。

**GB2312固定使用两个字节表示汉子，这两个字节最高位都是1.**

5. GBK

GBK是建立在GB2312基础上，向下兼容GB2312。GBK增加了14000多个汉字。

6. GB18030

GB18030向下兼容GBK，增加了55000多个字符。

**ASCII码是基础，使用一个字节表示，最高位为0，其他7位表示128个字符。其他编码都是兼容ASCII码的，最高位使用1来进行区分。**

### Unicode编码

- Unicode给世界上所有的字符都分配了一个唯一的数字编号，这个编号范围从0x000000~0x10FFFF.

- Unicode本身没有规定怎么把编码对应到二进制，把编号如何对应二进制主要有：

1. UTF-32 - 使用四个字节
2. UTF-16 - 大部分两个字节，少部分四个字节
3. UTF-8 - 使用1~4个字节

## char的真正含义

**Java中进行字符处理的基础是`char`，Java中的`Character`，`String`，`StringBuilder`的基础都是`char`。**

为什么字符可以进行算术运算和比较？

在Java内部进行字符处理时，采用的都是Unicode，具体的编码格式是UTF-16BE。

**char本质是一个固定占用两个字节的无符号正整数，这个正整数对应于Unicode编号，用于表示那个Unicode编号对应的字符。**

注意：

- char只能表示Unicode65536以内的字符，不能表示超出范围的字符。

因为UTF-16使用两个或者四个字节表示一个字符，Unicode编号范围在65356以内的占两个字节，超出范围占四个字节。

- 由于char本质是一个整数，所以可以进行整数能做的一些计算，在进行运算时会被看成int

- 输出char的二进制表示

```java
char d = '刘';
System.out.print(Integer.toBinaryString(d)); // print is 101001000011000
```
